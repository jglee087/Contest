{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUBLIC 20위 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 패키지 IMOPRT\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.optimizers import Adam,RMSprop,Nadam\n",
    "from tensorflow.python.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.python.keras.layers import Conv2D, Conv2DTranspose, Activation, Flatten, Dense, UpSampling2D, Reshape, Lambda,LSTM,Bidirectional, Permute,Multiply,Add,Concatenate,Dot\n",
    "from tensorflow.python.keras.layers import Input,BatchNormalization, Dropout, LeakyReLU,MaxPooling2D,Embedding,Conv1D,GlobalMaxPooling1D,MaxPooling1D,GaussianNoise,GaussianDropout,AlphaDropout\n",
    "from tensorflow.python.keras.losses import mean_absolute_error,mean_squared_error\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np \n",
    "#from pandas_profiling import ProfileReport\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 데이터를 불러옵니다.\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "# Test 데이터를 불러옵니다.\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피쳐 LIST를 만듭니다.\n",
    "feature_list=train.columns.to_list()\n",
    "feature_list.remove('id')\n",
    "feature_list.remove('type')\n",
    "feature_list.remove('fiberID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = train['type'].unique()\n",
    "label_dict = {val: i for i, val in enumerate(unique_labels)}\n",
    "i2lb = {v:k for k, v in label_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.iloc[:,3:]  #Train 데이터의 독립변수\n",
    "train_X_type=train.iloc[:,2]\n",
    "train_Y = train.iloc[:,1] #Train 데이터의 종속변수\n",
    "test_X = test.iloc[:,2:]    #Test  데이터의 독립변수\n",
    "test_X_type=test.iloc[:,1]\n",
    "test_ids = test['id']\n",
    "train_Y = train_Y.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일링을 합니다.\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,QuantileTransformer,RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X=scaler.transform(train_X)\n",
    "test_X=scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=pd.DataFrame(train_X)\n",
    "train_Y=pd.DataFrame(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold , train_test_split , KFold\n",
    "t_x,v_x,t_x_type,v_x_type,t_y,v_y = train_test_split(train_X,train_X_type,train_Y , stratify = train_Y , random_state = 730, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5*x*(1+K.tanh(np.sqrt(2/np.pi)*(x+0.044715*K.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss',patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjk_sch(epoch):\n",
    "    init_lr=0.003\n",
    "    min_lr=0.00003\n",
    "    e=150\n",
    "    step=np.exp(np.log(min_lr/init_lr)/e)\n",
    "    if(epoch<20):\n",
    "        return init_lr\n",
    "    else :\n",
    "        hjk_lr=init_lr*step**(epoch-20)\n",
    "        if(hjk_lr>min_lr):\n",
    "            return hjk_lr\n",
    "        else:\n",
    "            return min_lr\n",
    "\n",
    "hjk_scher = keras.callbacks.LearningRateScheduler(hjk_sch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num=20\n",
    "input_type_num=1000\n",
    "output_num=19\n",
    "\n",
    "i=Input(shape=(input_num,))\n",
    "i_type=Input(shape=(1,))\n",
    "out_type=Embedding(input_type_num,5)(i_type)\n",
    "out_type=Reshape((5,))(out_type)\n",
    "out=Concatenate()([i,out_type])\n",
    "out=Reshape((5,5))(out)\n",
    "out=LSTM(input_num,return_sequences=True)(out)\n",
    "out=Permute((2,1))(out)\n",
    "out=LSTM(input_num,return_sequences=True)(out)\n",
    "out=Permute((2,1))(out)\n",
    "out=Flatten()(out)\n",
    "out=Dense(output_num*20,activation=gelu)(out)\n",
    "out=Dense(output_num*15,activation=gelu)(out)\n",
    "out=Dense(output_num*10,activation=gelu)(out)\n",
    "out=Dense(output_num*5,activation=gelu)(out)\n",
    "out=Dense(output_num,activation='softmax')(out)\n",
    "hjk_DL_model=Model(inputs=[i,i_type],outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(hjk_DL_model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hjk_DL_model.compile(loss=['categorical_crossentropy'],metrics=['categorical_crossentropy'],optimizer=Adam())\n",
    "hjk_DL_model.fit([t_x,t_x_type],keras.utils.to_categorical(t_y), epochs=200,batch_size=3000,validation_data=[[v_x,v_x_type],keras.utils.to_categorical(v_y)],callbacks=[hjk_scher,es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_5_f(df :pd.DataFrame, i):\n",
    "    df[str(i) + '_add_' +str(i+4)]=df.iloc[:,i] + df.iloc[:,i+1] + df.iloc[:,i+2] + df.iloc[:,i+3] + df.iloc[:,i+4]\n",
    "    df[str(i) + '_mul_' +str(i+4)]=df.iloc[:,i] * df.iloc[:,i+1] * df.iloc[:,i+2] * df.iloc[:,i+3] * df.iloc[:,i+4]\n",
    "    df[str(i) + '_add_1' +str(i+4)]=df.iloc[:,i] + df.iloc[:,i+1]\n",
    "    df[str(i) + '_add_2' +str(i+4)]=df.iloc[:,i] + df.iloc[:,i+1]+df.iloc[:,i+2]\n",
    "    df[str(i) + '_add_3' +str(i+4)]=df.iloc[:,i] + df.iloc[:,i+1]+df.iloc[:,i+2]+ df.iloc[:,i+3]\n",
    "    df[str(i) + '_mul_1' +str(i+4)]=df.iloc[:,i] * df.iloc[:,i+1] \n",
    "    df[str(i) + '_mul_2' +str(i+4)]=df.iloc[:,i] * df.iloc[:,i+1] * df.iloc[:,i+2]\n",
    "    df[str(i) + '_mul_3' +str(i+4)]=df.iloc[:,i] * df.iloc[:,i+1] * df.iloc[:,i+2]* df.iloc[:,i+3]\n",
    "    df[str(i) + '_mean_' +str(i+4)]=np.mean([df.iloc[:,i] , df.iloc[:,i+1] , df.iloc[:,i+2] , df.iloc[:,i+3] , df.iloc[:,i+4]],axis=0)\n",
    "    df[str(i) + '_stb_' +str(i+4)]=np.std([df.iloc[:,i] , df.iloc[:,i+1] , df.iloc[:,i+2] , df.iloc[:,i+3] , df.iloc[:,i+4]],axis=0)\n",
    "    df[str(i) + '_max_' +str(i+4)]=np.max([df.iloc[:,i] , df.iloc[:,i+1] , df.iloc[:,i+2] , df.iloc[:,i+3] , df.iloc[:,i+4]],axis=0)\n",
    "    df[str(i) + '_min_' +str(i+4)]=np.max([df.iloc[:,i] , df.iloc[:,i+1] , df.iloc[:,i+2] , df.iloc[:,i+3] , df.iloc[:,i+4]],axis=0)\n",
    "\n",
    "def add_4_f(df :pd.DataFrame, i):\n",
    "    df[str(i) + '_add_' +str(i+15)]=df.iloc[:,i] + df.iloc[:,i+5] + df.iloc[:,i+10] + df.iloc[:,i+15]\n",
    "    df[str(i) + '_add_1' +str(i+15)]=df.iloc[:,i] + df.iloc[:,i+5] \n",
    "    df[str(i) + '_add_2' +str(i+15)]=df.iloc[:,i] + df.iloc[:,i+5] + df.iloc[:,i+10]\n",
    "    df[str(i) + '_mul_' +str(i+15)]=df.iloc[:,i] * df.iloc[:,i+5] * df.iloc[:,i+10] * df.iloc[:,i+15]\n",
    "    df[str(i) + '_mul_1' +str(i+15)]=df.iloc[:,i] * df.iloc[:,i+5]\n",
    "    df[str(i) + '_mul_2' +str(i+15)]=df.iloc[:,i] * df.iloc[:,i+5]* df.iloc[:,i+10]\n",
    "    df[str(i) + '_mean_' +str(i+15)]=np.mean([df.iloc[:,i] , df.iloc[:,i+5] , df.iloc[:,i+10] , df.iloc[:,i+15]],axis=0)\n",
    "    df[str(i) + '_stb_' +str(i+15)]=np.std([df.iloc[:,i] , df.iloc[:,i+5] , df.iloc[:,i+10] , df.iloc[:,i+15]],axis=0)\n",
    "\n",
    "def add_f(df :pd.DataFrame, i, j):\n",
    "    df[str(i) + '_add_' +str(j)]=df.iloc[:,i] + df.iloc[:,j]\n",
    "    df[str(i) + '_sub_' +str(j)]=np.abs(df.iloc[:,i] - df.iloc[:,j])\n",
    "    df[str(i) + '_mul_' +str(j)]=df.iloc[:,i] * df.iloc[:,j]\n",
    "    df[str(i) + '_dev_' +str(j)]=df.iloc[:,i] / (df.iloc[:,j] +1)\n",
    "    df[str(j) + '_dev_' +str(i)]=df.iloc[:,j] / (df.iloc[:,i] +1)\n",
    "\n",
    "def add_s_f(df :pd.DataFrame, i):\n",
    "    df['sin' + str(i)]=np.sin(df.iloc[:,i])\n",
    "    df['cos' + str(i)]=np.cos(df.iloc[:,i])\n",
    "    df['tan' + str(i)]=np.tan(df.iloc[:,i])\n",
    "    df['log' + str(i)]=np.log1p(np.abs(df.iloc[:,i]))\n",
    "    df['inv'+ str(i)]=1/(df.iloc[:,i]+1)\n",
    "    df['sqrt'+str(i)]=np.sqrt(df.iloc[:,i])\n",
    "    df['square'+str(i)]=np.square(df.iloc[:,i])\n",
    "    df['exp'+str(i)]=np.exp(df.iloc[:,i])\n",
    "    df['tanh'+str(i)]=np.tanh(df.iloc[:,i])\n",
    "    df['arccos'+str(i)]=np.arccos(df.iloc[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.iloc[:,3:]  #Train 데이터의 독립변수\n",
    "train_X_type=train.iloc[:,2]\n",
    "train_Y = train.iloc[:,1] #Train 데이터의 종속변수\n",
    "test_X = test.iloc[:,2:]    #Test  데이터의 독립변수\n",
    "test_X_type=test.iloc[:,1]\n",
    "test_ids = test['id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in feature_list:\n",
    "    min_value=train_X[f].min()\n",
    "    train_X[f]=train_X[f].apply(lambda x : np.log1p(x-min_value))\n",
    "    test_X[f]=test_X[f].apply(lambda x : np.log1p(x-min_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X['f_id']=train_X_type\n",
    "test_X['f_id']=test_X_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_X)\n",
    "train_X=scaler.transform(train_X)\n",
    "test_X=scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=pd.DataFrame(train_X)\n",
    "test_X=pd.DataFrame(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_Y.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    add_s_f(train_X,i)\n",
    "    add_f(train_X,i,20)\n",
    "    add_s_f(test_X,i)\n",
    "    add_f(test_X,i,20)\n",
    "\n",
    "for i in range(4):\n",
    "    add_f(train_X,(i*5),(i*5)+1)\n",
    "    add_f(train_X,(i*5),(i*5)+2)\n",
    "    add_f(train_X,(i*5),(i*5)+3)\n",
    "    add_f(train_X,(i*5),(i*5)+4)\n",
    "    add_f(train_X,(i*5)+1,(i*5)+2)\n",
    "    add_f(train_X,(i*5)+1,(i*5)+3)\n",
    "    add_f(train_X,(i*5)+1,(i*5)+4)\n",
    "    add_f(train_X,(i*5)+2,(i*5)+3)\n",
    "    add_f(train_X,(i*5)+2,(i*5)+4)\n",
    "    add_f(train_X,(i*5)+3,(i*5)+4)\n",
    "    \n",
    "    add_f(test_X,(i*5),(i*5)+1)\n",
    "    add_f(test_X,(i*5),(i*5)+2)\n",
    "    add_f(test_X,(i*5),(i*5)+3)\n",
    "    add_f(test_X,(i*5),(i*5)+4)\n",
    "    add_f(test_X,(i*5)+1,(i*5)+2)\n",
    "    add_f(test_X,(i*5)+1,(i*5)+3)\n",
    "    add_f(test_X,(i*5)+1,(i*5)+4)\n",
    "    add_f(test_X,(i*5)+2,(i*5)+3)\n",
    "    add_f(test_X,(i*5)+2,(i*5)+4)\n",
    "    add_f(test_X,(i*5)+3,(i*5)+4)\n",
    "\n",
    "    add_5_f(train_X,(i*5))\n",
    "    add_5_f(test_X,(i*5))\n",
    "\n",
    "for i in range(5):\n",
    "    add_f(train_X,i,i+5)\n",
    "    add_f(train_X,i,i+10)\n",
    "    add_f(train_X,i,i+15)\n",
    "    add_f(train_X,i+5,i+10)\n",
    "    add_f(train_X,i+5,i+15)\n",
    "    add_f(train_X,i+10,i+15)\n",
    "\n",
    "    add_f(test_X,i,i+5)\n",
    "    add_f(test_X,i,i+10)\n",
    "    add_f(test_X,i,i+15)\n",
    "    add_f(test_X,i+5,i+10)\n",
    "    add_f(test_X,i+5,i+15)\n",
    "    add_f(test_X,i+10,i+15)\n",
    "\n",
    "    add_4_f(train_X,(i))\n",
    "    add_4_f(test_X,(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.min().min())\n",
    "print(train_X.max().max())\n",
    "t=train_X.isnull().sum()\n",
    "t[t>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_x,v_x,t_x_type,v_x_type,t_y,v_y = train_test_split(train_X,train_X_type,train_Y , stratify = train_Y , random_state = 1000, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjk_lr_sch(current_iter):\n",
    "    init_lr=0.1\n",
    "    min_lr=0.001\n",
    "    e=1000\n",
    "    step=np.exp(np.log(min_lr/init_lr)/e)\n",
    "    if(current_iter<5):\n",
    "        return init_lr\n",
    "    else :\n",
    "        hjk_lr=init_lr*step**(current_iter-5)\n",
    "        if(hjk_lr>min_lr):\n",
    "            return hjk_lr\n",
    "        else:\n",
    "            return min_lr\n",
    "\n",
    "def hjk_lr_sch_2(current_iter):\n",
    "    init_lr=0.07\n",
    "    min_lr=0.03\n",
    "    hjk_lr=init_lr*np.power(0.998,current_iter)\n",
    "    if(hjk_lr>min_lr):\n",
    "        return hjk_lr\n",
    "    else:\n",
    "        return min_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_clf = LGBMClassifier(\n",
    "    boosting_type='gbdt',\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=160,\n",
    "    max_depth=7,\n",
    "    min_child_samples=20,\n",
    "    min_child_weight=2,\n",
    "    random_state=1,\n",
    "\n",
    "    colsample_bytree=0.9,\n",
    "    sub_sample=0.9,\n",
    "    # subsample_freq=0.,\n",
    "    reg_alpha=0.,\n",
    "    reg_lambda=0.,\n",
    "    importance_type='gain',\n",
    "    tree_method='gpu_hist'\n",
    ")\n",
    "\n",
    "lgbm_clf.fit(t_x, t_y,eval_set=(v_x,v_y),early_stopping_rounds=20,callbacks=[lightgbm.reset_parameter(learning_rate=hjk_lr_sch_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mat = hjk_model.predict(test_X)\n",
    "sample = pd.read_csv('/gdrive/My Drive/SPACE/sample_submission.csv')\n",
    "submission = pd.DataFrame(pred_mat, index=test.index)\n",
    "submission = submission.rename(columns=i2lb)\n",
    "submission = pd.concat([test_ids, submission], axis=1)\n",
    "submission = submission[sample.columns]\n",
    "submission.to_csv(\"/gdrive/My Drive/SPACE/hjk_space_10_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
