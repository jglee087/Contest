{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import keras \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras_radam import RAdam\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "random.seed(7)\n",
    "#tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = pd.read_csv('./data/train.csv',index_col='id')\n",
    "test_ = pd. read_csv('./data/test.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train_.copy()\n",
    "test=test_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기상청 데이터만 추출\n",
    "x_train = train.loc[:,'X00':'X39']\n",
    "\n",
    "# standardization을 위해 평균과 표준편차 구하기\n",
    "MEAN = x_train.mean()\n",
    "STD = x_train.std()\n",
    "\n",
    "# 표준편차가 0일 경우 대비하여 1e-07 추가 \n",
    "x_train = (x_train - MEAN) / (STD + 1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 모델에 입력 할 수 있는 시계열 형태로 데이터 변환 \n",
    "def convert_to_timeseries(df, interval):\n",
    "    sequence_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    for i in tqdm(range(df.shape[0] - interval)):\n",
    "        sequence_list.append(np.array(df.iloc[i:i+interval,:-1]))\n",
    "        target_list.append(df.iloc[i+interval,-1])\n",
    "    \n",
    "    sequence = np.array(sequence_list)\n",
    "    target = np.array(target_list)\n",
    "    \n",
    "    return sequence, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = ['Y15','Y16']\n",
    "#y_columns= train.iloc[:,40:-1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4308/4308 [00:01<00:00, 2621.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 4308/4308 [00:01<00:00, 2505.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# t시점 이전 120분의 데이터로 t시점의 온도를 추정할 수 있는 학습데이터 형성\n",
    "sequence = np.empty((0, 12, 40))\n",
    "target = np.empty((0,))\n",
    "for column in y_columns :\n",
    "    \n",
    "    concat = pd.concat([x_train, train[column]], axis = 1)\n",
    "\n",
    "    _sequence, _target = convert_to_timeseries(concat.head(144*30), interval = 12)\n",
    "\n",
    "    sequence = np.vstack((sequence, _sequence))\n",
    "    target = np.hstack((target, _target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_timeseries 함수를 쓰기 위한 dummy feature 생성\n",
    "x_train['dummy'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set에서 도출된 평균과 표준편차로 standardization 실시 \n",
    "test = (test - MEAN) / (STD + 1e-07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_timeseries 함수를 쓰기 위한 dummy feature 생성\n",
    "test['dummy'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 16260/16260 [00:06<00:00, 2337.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# train과 test 기간을 합쳐서 120분 간격으로 학습데이터 재구축\n",
    "x_test, _ = convert_to_timeseries(pd.concat([x_train, test], axis = 0), interval=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set 기간인 후반부 80일에 맞게 자르기 \n",
    "x_test = x_test[-11520:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만들어 두었던 dummy feature 제거\n",
    "x_train.drop('dummy', axis = 1, inplace = True)\n",
    "test.drop('dummy', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 lstm 모델 구축하기 \n",
    "simple_lstm_model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(32, input_shape=sequence.shape[-2:]),\n",
    "    keras.layers.Dense(64, activation='elu'),\n",
    "    keras.layers.Dense(32, activation='elu'),    \n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "simple_lstm_model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss가 4미만으로 떨어지면 학습 종료 시키는 기능\n",
    "class myCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        if(logs.get('loss') < 0.0001):\n",
    "            print('\\n Loss is under 0.0001, cancelling training')\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 간단한 lstm 모델 구축하기 \n",
    "# simple_lstm_model = keras.models.Sequential([\n",
    "#     keras.layers.LSTM(64, input_shape=sequence.shape[-2:]),\n",
    "#     keras.layers.Dense(64, activation='elu'),\n",
    "#     keras.layers.Dense(32, activation='elu'),    \n",
    "#     keras.layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "# simple_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "# # 모델 학습\n",
    "# simple_lstm_model.fit(sequence, target, epochs=60, batch_size=128, verbose=2,\n",
    "#     shuffle=False, callbacks = [callbacks], validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 간단한 lstm 모델 구축하기 \n",
    "# simple_lstm_model = keras.models.Sequential([\n",
    "#     keras.layers.LSTM(32, input_shape=sequence.shape[-2:]),\n",
    "#     keras.layers.Dense(64, activation='elu'),\n",
    "#     keras.layers.Dense(32, activation='elu'),    \n",
    "#     keras.layers.Dense(1)\n",
    "# ])\n",
    "# simple_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "# # 모델 학습\n",
    "# simple_lstm_model.fit(sequence, target, epochs=80, batch_size=128, verbose=2,\n",
    "#     shuffle=False, callbacks = [callbacks], validation_split=0.1)\n",
    "\n",
    "# Epoch 80/80\n",
    "#  - 1s - loss: 2.5424 - val_loss: 2.6561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 간단한 lstm 모델 구축하기 \n",
    "# simple_lstm_model = keras.models.Sequential([\n",
    "#     keras.layers.LSTM(64, input_shape=sequence.shape[-2:], activation='tanh'),\n",
    "#     keras.layers.Dense(64, activation='elu'),\n",
    "#     keras.layers.Dense(64, activation='elu'),\n",
    "#     keras.layers.Dense(64, activation='elu'),\n",
    "#     keras.layers.Dense(64, activation='elu'),    \n",
    "#     keras.layers.Dropout(0.05),\n",
    "#     keras.layers.Dense(1)\n",
    "# ])\n",
    "# simple_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "# # 모델 학습\n",
    "# simple_lstm_model.fit(sequence, target, epochs=80, batch_size=128, verbose=2,\n",
    "#     shuffle=False, callbacks = [callbacks], validation_split=0.1)\n",
    "\n",
    "# Epoch 80/80\n",
    "# - 1s - loss: 3.1128 - val_loss: 3.5588"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 간단한 lstm 모델 구축하기 \n",
    "# simple_lstm_model = keras.models.Sequential([\n",
    "#     keras.layers.LSTM(64, input_shape=sequence.shape[-2:], activation='tanh', return_sequences=True),\n",
    "#     keras.layers.LSTM(64),\n",
    "#     keras.layers.Dense(64, activation='elu'),    \n",
    "#     keras.layers.Dropout(0.05),\n",
    "#     keras.layers.Dense(1)\n",
    "# ])\n",
    "# simple_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "# # 모델 학습\n",
    "# simple_lstm_model.fit(sequence, target, epochs=80, batch_size=128, verbose=2,\n",
    "#     shuffle=False, callbacks = [callbacks], validation_split=0.1)\n",
    "\n",
    "# Epoch 79/80\n",
    "#  - 2s - loss: 2.8924 - val_loss: 3.3730\n",
    "# Epoch 80/80\n",
    "#  - 2s - loss: 2.7272 - val_loss: 3.1412\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 간단한 lstm 모델 구축하기 \n",
    "# simple_lstm_model = keras.models.Sequential([\n",
    "#     keras.layers.LSTM(64, input_shape=sequence.shape[-2:], activation='tanh', return_sequences=True),\n",
    "#     keras.layers.LSTM(64),\n",
    "#     keras.layers.Dense(64, activation='elu'),    \n",
    "#     keras.layers.Dropout(0.05),\n",
    "#     keras.layers.Dense(1)\n",
    "# ])\n",
    "# simple_lstm_model.compile(optimizer='adam', loss='mse')\n",
    "# # 모델 학습\n",
    "# simple_lstm_model.fit(sequence, target, epochs=80, batch_size=32, verbose=2,\n",
    "#     shuffle=False, callbacks = [callbacks], validation_split=0.1)\n",
    "\n",
    "# Epoch 78/80\n",
    "#  - 4s - loss: 2.0523 - val_loss: 2.3489\n",
    "# Epoch 79/80\n",
    "#  - 4s - loss: 2.2302 - val_loss: 2.2888\n",
    "# Epoch 80/80\n",
    "#  - 4s - loss: 2.0973 - val_loss: 3.1370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7754 samples, validate on 862 samples\n",
      "Epoch 1/80\n",
      " - 5s - loss: 523.9605 - val_loss: 579.0571\n",
      "Epoch 2/80\n",
      " - 4s - loss: 448.9724 - val_loss: 494.2359\n",
      "Epoch 3/80\n",
      " - 4s - loss: 281.8172 - val_loss: 305.9006\n",
      "Epoch 4/80\n",
      " - 5s - loss: 115.0034 - val_loss: 138.7109\n",
      "Epoch 5/80\n",
      " - 4s - loss: 51.7986 - val_loss: 51.8027\n",
      "Epoch 6/80\n",
      " - 4s - loss: 27.6234 - val_loss: 25.0665\n",
      "Epoch 7/80\n",
      " - 4s - loss: 21.2916 - val_loss: 18.2466\n",
      "Epoch 8/80\n",
      " - 4s - loss: 17.6657 - val_loss: 14.9382\n",
      "Epoch 9/80\n",
      " - 4s - loss: 15.1468 - val_loss: 12.5885\n",
      "Epoch 10/80\n",
      " - 4s - loss: 12.8736 - val_loss: 10.5851\n",
      "Epoch 11/80\n",
      " - 5s - loss: 11.4787 - val_loss: 9.6024\n",
      "Epoch 12/80\n",
      " - 5s - loss: 11.0108 - val_loss: 9.0160\n",
      "Epoch 13/80\n",
      " - 5s - loss: 10.3651 - val_loss: 8.6302\n",
      "Epoch 14/80\n",
      " - 4s - loss: 9.9155 - val_loss: 8.3662\n",
      "Epoch 15/80\n",
      " - 5s - loss: 9.5926 - val_loss: 8.1205\n",
      "Epoch 16/80\n",
      " - 4s - loss: 9.3343 - val_loss: 7.9701\n",
      "Epoch 17/80\n",
      " - 4s - loss: 9.0352 - val_loss: 7.9211\n",
      "Epoch 18/80\n",
      " - 4s - loss: 8.9633 - val_loss: 7.8372\n",
      "Epoch 19/80\n",
      " - 4s - loss: 8.8354 - val_loss: 7.7635\n",
      "Epoch 20/80\n",
      " - 5s - loss: 8.6975 - val_loss: 7.6081\n",
      "Epoch 21/80\n",
      " - 5s - loss: 8.5549 - val_loss: 7.6600\n",
      "Epoch 22/80\n",
      " - 5s - loss: 8.1088 - val_loss: 7.6259\n",
      "Epoch 23/80\n",
      " - 4s - loss: 8.1999 - val_loss: 7.6373\n",
      "Epoch 24/80\n",
      " - 4s - loss: 7.9674 - val_loss: 7.5305\n",
      "Epoch 25/80\n",
      " - 4s - loss: 7.9297 - val_loss: 7.4340\n",
      "Epoch 26/80\n",
      " - 4s - loss: 7.9287 - val_loss: 7.4326\n",
      "Epoch 27/80\n",
      " - 4s - loss: 7.9044 - val_loss: 7.2953\n",
      "Epoch 28/80\n",
      " - 5s - loss: 7.6852 - val_loss: 7.3025\n",
      "Epoch 29/80\n",
      " - 5s - loss: 7.6599 - val_loss: 7.2305\n",
      "Epoch 30/80\n",
      " - 5s - loss: 7.6200 - val_loss: 7.2517\n",
      "Epoch 31/80\n",
      " - 5s - loss: 7.4485 - val_loss: 7.1628\n",
      "Epoch 32/80\n",
      " - 5s - loss: 7.5862 - val_loss: 7.0961\n",
      "Epoch 33/80\n",
      " - 5s - loss: 7.5171 - val_loss: 7.1532\n",
      "Epoch 34/80\n",
      " - 5s - loss: 7.4847 - val_loss: 7.0784\n",
      "Epoch 35/80\n",
      " - 7s - loss: 7.3437 - val_loss: 6.9638\n",
      "Epoch 36/80\n",
      " - 6s - loss: 7.2561 - val_loss: 6.9271\n",
      "Epoch 37/80\n",
      " - 5s - loss: 7.1953 - val_loss: 6.7958\n",
      "Epoch 38/80\n",
      " - 5s - loss: 7.0933 - val_loss: 6.8972\n",
      "Epoch 39/80\n",
      " - 5s - loss: 6.9663 - val_loss: 6.7089\n",
      "Epoch 40/80\n",
      " - 5s - loss: 7.0440 - val_loss: 6.6975\n",
      "Epoch 41/80\n",
      " - 4s - loss: 7.0738 - val_loss: 6.6487\n",
      "Epoch 42/80\n",
      " - 4s - loss: 7.0026 - val_loss: 6.5978\n",
      "Epoch 43/80\n",
      " - 4s - loss: 6.9884 - val_loss: 6.5690\n",
      "Epoch 44/80\n",
      " - 4s - loss: 6.8062 - val_loss: 6.5232\n",
      "Epoch 45/80\n",
      " - 4s - loss: 6.8096 - val_loss: 6.5625\n",
      "Epoch 46/80\n",
      " - 4s - loss: 6.8894 - val_loss: 6.4788\n",
      "Epoch 47/80\n",
      " - 4s - loss: 6.8907 - val_loss: 6.4839\n",
      "Epoch 48/80\n",
      " - 4s - loss: 6.7455 - val_loss: 6.4565\n",
      "Epoch 49/80\n",
      " - 4s - loss: 6.7582 - val_loss: 6.4150\n",
      "Epoch 50/80\n",
      " - 4s - loss: 6.5786 - val_loss: 6.3652\n",
      "Epoch 51/80\n",
      " - 4s - loss: 6.6949 - val_loss: 6.4119\n",
      "Epoch 52/80\n",
      " - 4s - loss: 6.5697 - val_loss: 6.3204\n",
      "Epoch 53/80\n",
      " - 4s - loss: 6.5981 - val_loss: 6.2911\n",
      "Epoch 54/80\n",
      " - 4s - loss: 6.4660 - val_loss: 6.2501\n",
      "Epoch 55/80\n",
      " - 4s - loss: 6.5187 - val_loss: 6.2720\n",
      "Epoch 56/80\n",
      " - 5s - loss: 6.3458 - val_loss: 6.1890\n",
      "Epoch 57/80\n",
      " - 4s - loss: 6.4175 - val_loss: 6.1955\n",
      "Epoch 58/80\n",
      " - 4s - loss: 6.4822 - val_loss: 6.2221\n",
      "Epoch 59/80\n",
      " - 4s - loss: 6.3758 - val_loss: 6.2123\n",
      "Epoch 60/80\n",
      " - 5s - loss: 6.3720 - val_loss: 6.1397\n",
      "Epoch 61/80\n",
      " - 5s - loss: 6.2418 - val_loss: 6.0125\n",
      "Epoch 62/80\n",
      " - 5s - loss: 6.1746 - val_loss: 5.9845\n",
      "Epoch 63/80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-8cacd0514ad0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m simple_lstm_model.fit(sequence, target, epochs=80, batch_size=256, verbose=2,\n\u001b[1;32m---> 12\u001b[1;33m     shuffle=False, callbacks = [callbacks], validation_split=0.1)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # 간단한 lstm 모델 구축하기 \n",
    "# simple_lstm_model = keras.models.Sequential([\n",
    "#     keras.layers.LSTM(256, input_shape=sequence.shape[-2:], activation='tanh'),\n",
    "# #    keras.layers.LSTM(64, activation='tanh'),\n",
    "#     keras.layers.Dense(256, activation='elu'),\n",
    "#     keras.layers.Dropout(0.2),\n",
    "#     keras.layers.Dense(1)\n",
    "# ])\n",
    "# simple_lstm_model.compile(optimizer=Adam(lr=1.e-4), loss='mse')\n",
    "# # 모델 학습\n",
    "# simple_lstm_model.fit(sequence, target, epochs=80, batch_size=256, verbose=2,\n",
    "#     shuffle=False, callbacks = [callbacks], validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 레이어는 고정\n",
    "simple_lstm_model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning 할 때 사용할 학습데이터 생성 (Y18)\n",
    "finetune_X, finetune_y = convert_to_timeseries(pd.concat([x_train.tail(432), train['Y18'].tail(432)], axis = 1), interval=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM 레이어는 고정 시켜두고, DNN 레이어에 대해서 fine tuning 진행 (Transfer Learning)\n",
    "# finetune_history = simple_lstm_model.fit( finetune_X, finetune_y, epochs=20,\n",
    "#             batch_size=64, shuffle=False, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하기 \n",
    "finetune_pred = simple_lstm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 만들기\n",
    "submit = pd.DataFrame({'id':range(144*33, 144*113),\n",
    "              'Y18':finetune_pred.reshape(1,-1)[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('baseline_result.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
